{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501c23b-7302-4f6f-831b-b1e983eb07a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/18 18:31:16 WARN Utils: Your hostname, Kavanas-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.1.11 instead (on interface en0)\n",
      "25/11/18 18:31:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/18 18:31:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/18 18:31:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/18 18:31:22 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Transform\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb608a67-be51-4380-bc7f-74a9dc525412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 01:57:23 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw.\n",
      "java.io.FileNotFoundException: File /Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "UpWithPruning(AnalysisHelper.scala:135).logical.AnalysisHelper.resolveOperators\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "nalyzed$2(QueryExecution.scala:110)QueryExecution.$anonfun$lazyA\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1623)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviews_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/readwriter.py:642\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    631\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    633\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    634\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    640\u001b[0m )\n\u001b[0;32m--> 642\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mparquet(_to_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc, paths)))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "reviews_df = spark.read.parquet(\"/Users/kavanamanvi/Desktop/AmazonReviews/processed/bronze/reviews_raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026d7467-c2ec-4ea2-aa3a-542d334c1163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n",
      "|      asin|helpful_vote|images|parent_asin|rating|                text|    timestamp|               title|             user_id|verified_purchase|\n",
      "+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n",
      "|B004H0J5QY|           0|    []| B004HD55V0|   5.0|Great game for th...|1398016704000|Great game for th...|AHP4ABT4AOUOKHKXC...|             true|\n",
      "|B00AR5BNYU|           1|    []| B00AR5BNYU|   5.0|Great graphics,no...|1370048921000|Great value,for t...|AHP4ABT4AOUOKHKXC...|            false|\n",
      "|B003MQMPD4|           4|    []| B003NE6BQW|   1.0|I wish I could gi...|1326939063000|Do not waste the ...|AHP4ABT4AOUOKHKXC...|             true|\n",
      "|B003L77ZHK|           3|    []| B003L77ZHK|   4.0|I have been looki...|1326502126000|If looking for a ...|AHP4ABT4AOUOKHKXC...|             true|\n",
      "|B07TL5TKG9|           0|    []| B07WRDYD2N|   5.0|Fit perfectly wit...|1612121491449|  Just what I needed|AEHINTI4PJFF3EQ7O...|             true|\n",
      "+----------+------------+------+-----------+------+--------------------+-------------+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "reviews_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41deef1a-989a-4f40-ab87-6169ce215956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful_vote: long (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- attachment_type: string (nullable = true)\n",
      " |    |    |-- large_image_url: string (nullable = true)\n",
      " |    |    |-- medium_image_url: string (nullable = true)\n",
      " |    |    |-- small_image_url: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b875cff-ca02-4b77-8769-1c5c0ac44bfc",
   "metadata": {},
   "source": [
    "## 2B) Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "241a396c-fff6-426d-b7bf-2ea2d0d2b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting only the columns I will need for Recc algo\n",
    "clean_df = reviews_df.select(\"user_id\", \"asin\", \"rating\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bdc6532-c4a5-4a1f-b983-9c0ac0932ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+-------------+\n",
      "|             user_id|      asin|rating|    timestamp|\n",
      "+--------------------+----------+------+-------------+\n",
      "|AHP4ABT4AOUOKHKXC...|B004H0J5QY|   5.0|1398016704000|\n",
      "|AHP4ABT4AOUOKHKXC...|B00AR5BNYU|   5.0|1370048921000|\n",
      "|AHP4ABT4AOUOKHKXC...|B003MQMPD4|   1.0|1326939063000|\n",
      "|AHP4ABT4AOUOKHKXC...|B003L77ZHK|   4.0|1326502126000|\n",
      "|AEHINTI4PJFF3EQ7O...|B07TL5TKG9|   5.0|1612121491449|\n",
      "+--------------------+----------+------+-------------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_df.show(5)\n",
    "clean_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ab11a-7477-4a9a-806b-47e71f8d35b6",
   "metadata": {},
   "source": [
    "### Step 2B.2)Remove rows with missing key values\n",
    "\n",
    "Why?\n",
    "\n",
    "Because:\n",
    "\n",
    "user_id cannot be null\n",
    "\n",
    "asin cannot be null\n",
    "\n",
    "rating cannot be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "096f9921-46e7-4667-876c-85fb39bd57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.dropna(subset=[\"user_id\", \"asin\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b49d3345-f5dd-4a27-b5d9-cfd710e7d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+-------------+\n",
      "|             user_id|      asin|rating|    timestamp|\n",
      "+--------------------+----------+------+-------------+\n",
      "|AHP4ABT4AOUOKHKXC...|B004H0J5QY|   5.0|1398016704000|\n",
      "|AHP4ABT4AOUOKHKXC...|B00AR5BNYU|   5.0|1370048921000|\n",
      "|AHP4ABT4AOUOKHKXC...|B003MQMPD4|   1.0|1326939063000|\n",
      "|AHP4ABT4AOUOKHKXC...|B003L77ZHK|   4.0|1326502126000|\n",
      "|AEHINTI4PJFF3EQ7O...|B07TL5TKG9|   5.0|1612121491449|\n",
      "+--------------------+----------+------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "clean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0048103-09cb-4dae-85e7-db4d228c2b9a",
   "metadata": {},
   "source": [
    "### Step 2B.3) Convert timestamp into real date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20860d4e-b54f-487d-bd30-2035bebfd8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, to_timestamp\n",
    "\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"event_time\",\n",
    "    to_timestamp(from_unixtime(\"timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d989437a-f78d-4ebc-a74b-6898aa6a1bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+-------------+--------------------+\n",
      "|             user_id|      asin|rating|    timestamp|          event_time|\n",
      "+--------------------+----------+------+-------------+--------------------+\n",
      "|AHP4ABT4AOUOKHKXC...|B004H0J5QY|   5.0|1398016704000|+46271-05-21 16:2...|\n",
      "|AHP4ABT4AOUOKHKXC...|B00AR5BNYU|   5.0|1370048921000|+45385-02-13 10:4...|\n",
      "|AHP4ABT4AOUOKHKXC...|B003MQMPD4|   1.0|1326939063000|+44019-01-09 18:1...|\n",
      "|AHP4ABT4AOUOKHKXC...|B003L77ZHK|   4.0|1326502126000|+44005-03-06 14:4...|\n",
      "|AEHINTI4PJFF3EQ7O...|B07TL5TKG9|   5.0|1612121491449|+53056-02-05 07:2...|\n",
      "+--------------------+----------+------+-------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "clean_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab465582-c859-4b0c-aabd-0d913d3838fb",
   "metadata": {},
   "source": [
    "### Step 2B.4) Inspect the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4613d1da-4169-44cf-858a-224e14be3c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      "\n",
      "+--------------------+----------+------+-------------+--------------------+\n",
      "|             user_id|      asin|rating|    timestamp|          event_time|\n",
      "+--------------------+----------+------+-------------+--------------------+\n",
      "|AHP4ABT4AOUOKHKXC...|B004H0J5QY|   5.0|1398016704000|+46271-05-21 16:2...|\n",
      "|AHP4ABT4AOUOKHKXC...|B00AR5BNYU|   5.0|1370048921000|+45385-02-13 10:4...|\n",
      "|AHP4ABT4AOUOKHKXC...|B003MQMPD4|   1.0|1326939063000|+44019-01-09 18:1...|\n",
      "|AHP4ABT4AOUOKHKXC...|B003L77ZHK|   4.0|1326502126000|+44005-03-06 14:4...|\n",
      "|AEHINTI4PJFF3EQ7O...|B07TL5TKG9|   5.0|1612121491449|+53056-02-05 07:2...|\n",
      "|AFVOXOECS5WI3ZWDO...|B00BT9DTC2|   5.0|1464131745000|+48366-06-28 01:3...|\n",
      "|AEI5JLLF4OUBDQSBF...|B00K586O7K|   1.0|1410276567000|+46659-11-20 04:5...|\n",
      "|AFR75TQBIL7AJJV3O...|B004YCRITQ|   5.0|1636568167527|+53830-10-12 23:4...|\n",
      "|AG4GXTN7KEWAX3FRC...|B07XQXZXJC|   1.0|1583332599907|+52143-10-24 14:0...|\n",
      "|AG4GXTN7KEWAX3FRC...|B07XQXZXJC|   1.0|1583332599907|+52143-10-24 14:0...|\n",
      "+--------------------+----------+------+-------------+--------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "4624615"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.printSchema()\n",
    "clean_df.show(10)\n",
    "clean_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2911e2b-7d5c-4a46-9477-571f76ed2d26",
   "metadata": {},
   "source": [
    "### Step 2B.5) Save CLEANED data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc66089f-1e1c-420b-a256-3b7f4d18ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "clean_df.write.mode(\"overwrite\").parquet(\"/Users/kavanamanvi/Desktop/AmazonReviews/processed/silver/reviews_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e698cf-bfbb-4ec3-98e0-06ab53c4714e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
